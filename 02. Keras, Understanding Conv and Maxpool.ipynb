{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook:\n",
    "- What is convolution and maxpooling? \n",
    "- What are convnets? \n",
    "- What do convnets learn? \n",
    "\n",
    "### In previous notebook: \n",
    "- MNIST Demo\n",
    "    - Using Convolution Layers\n",
    "- Code Overview\n",
    "\n",
    "### In next notebook:\n",
    "- Training your own small convnets from scratch\n",
    "- Using data augmentation to mitigate overfitting\n",
    "- Using a pre-trained convnet to do feature extraction\n",
    "- Fine-tuning a pre-trained convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 12, 10)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import date\n",
    "date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NirantK. https://github.com/NirantK/keras-practice\n"
     ]
    }
   ],
   "source": [
    "author = \"NirantK. https://github.com/NirantK/keras-practice\"\n",
    "print(author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.8'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are on Windows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if os.name=='nt':\n",
    "    print('We are on Windows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start by understanding the code we saw in MNIST Demo**\n",
    "\n",
    "Here is the code again for your reference:\n",
    "\n",
    "```python\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://adeshpande3.github.io/assets/Cover.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Line 1\n",
    "\n",
    "```python\n",
    "model = model.Sequential()\n",
    "```\n",
    "Keras supports two different API flavours. The first is the sequential version. This works best for linear stacks of layers, which is the most common network architecture by far, and the *functional API* - for directed acyclic graphs of layers, allowing to build completely arbitrary architectures.\n",
    "\n",
    "For the forseeable examples, we will focus on ```Sequential``` models only, but for your reference here are two code sample doing exactly the same thing in Sequential and functional API both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sequential Model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model_sequential = models.Sequential()\n",
    "model_sequential.add(layers.Dense(32, activation='relu', input_shape=(784,)))\n",
    "model_sequential.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nirantk\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Same model as above in Functional API\n",
    "input_tensor = layers.Input(shape=(784,))\n",
    "x = layers.Dense(32, activation='relu')(input_tensor)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model_functional_api = models.Model(input=input_tensor, output=output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line 2\n",
    "```python\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "```\n",
    "In the model object, we are adding more objects using ```add()```. \n",
    "\n",
    "Here, we are adding the ```layer``` object where the object type is ```Conv2D```. We will configure our convnet to process inputs of size ```(28, 28, 1)```, which is the format of MNIST images. We do this via passing the argument ```input_shape=(28, 28, 1)``` to our first layer.\n",
    "\n",
    "Importantly, a convnet takes as input tensors of shape ```(image_height, image_width, image_channels)``` (not including the batch dimension). \n",
    "\n",
    "**Why ```Conv2D```?**\n",
    "Our densely-connected network earlier had a test accuracy of 97.8%, our basic convnet has a test accuracy of 99%+. This highlights the practical importance of convolution operation. \n",
    "\n",
    "Let's dive deeper into ```Conv2D``` and ```MaxPooling2D``` operations. \n",
    "Convolution layers learn local patterns, i.e. in the case of images, patterns found in small 2D windows of the inputs. In our example above, these windows were all 3x3.\n",
    "\n",
    "![](https://brohrer.github.io/images/cnn6.png)\n",
    "*Figure from [How CNNs work?](https://brohrer.github.io/how_convolutional_neural_networks_work.html)*\n",
    "\n",
    "Quick aside on Strides and Convolution Operation: \n",
    "\n",
    "![](http://deeplearning.net/software/theano/_images/numerical_padding_strides.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is Convolution effective for images?**\n",
    "- **Translation Invariance**: The patterns they learn are *translation-invariant*, i.e. after learning a certain pattern in the bottom right corner of a picture, a convnet is able to recognize it anywhere, e.g. in the top left corner\n",
    "- **Spatially Hierarchical Features**: \n",
    "A first convolution layer will learn small local patterns such as edges, but a second convolution layer will learn larger patterns made of the features of the first layers. And so on. This allows convnets to efficiently learn increasingly complex and abstract visual concept\n",
    "![](https://dpzbhybb2pdcj.cloudfront.net/chollet/v-6/Figures/visual_hierarchy_rsd.jpg)\n",
    "\n",
    "*Figure from Deep Learning with Python by F. Chollet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutions are defined by two key parameters:\n",
    "\n",
    "- The size of the patches that are extracted from the inputs (typically 3x3 or 5x5). In our example it was always 3x3, which is a very common choice.\n",
    "\n",
    "- The depth of the output feature map, i.e. the number of filters computed by the convolution. In our example, we started with a depth of 32 and ended with a depth of 64.\n",
    "\n",
    "In Keras Conv2D layers, these parameters are the first arguments passed to the layer: ```Conv2D(output_depth, (window_height, window_width))```\n",
    "\n",
    "---\n",
    "\n",
    "Trivia: What do these layers actually learn? How can you check that? Look for layer/neuron activations across multiple images\n",
    "\n",
    "![](https://brohrer.github.io/images/cnn18.png)\n",
    "Higher layers can represent increasingly sophisticated aspects of the image, such as shapes and patterns. These tend to be readily recognizable. For instance, in a CNN trained on human faces, the highest layers represent patterns that are clearly face-like.\n",
    "\n",
    "*Figure from [Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations](http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Max pooling operation**\n",
    "\n",
    "MaxPooling2D layer halves the size of feature map every time.\n",
    "So, we go from 26X26 to 13X13 to \n",
    "\n",
    "In our convnet example, you may have noticed that the size of the feature maps gets halved after every  . For instance, before the first MaxPooling2D layers, the feature map is 26x26, but the max pooling operation halves it to 13x13. Thatâ€™s the role of max pooling: to aggressively downsample feature maps, much like strided convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://adeshpande3.github.io/assets/LeNet.png)\n",
    "*Figure Courtesy: [Adesh Pande](https://adeshpande3.github.io), UCLA '19* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other downsampling strategies could we use? \n",
    "- Use strides in the previous convolution layer\n",
    "- Use average pooling instead of max pooling, where each local input patch is transformed by taking the average value of each channel over the patch, rather than the max. \n",
    "\n",
    "However, max pooling tends to work better than these alternative solutions. \n",
    "\n",
    "In a nutshell, the reason for this is that features tend to encode the spatial \"presence\" of some pattern. These are learnt/encdoded over the different tiles of the feature map (hence the term \"feature map\"). This make it much more informative to look at the maximal presence of different features than at their average presence. \n",
    "\n",
    "So the most reasonable subsampling strategy is to first produce dense maps of features (via unstrided convolutions) and then look at the maximal activation of the features over small patches, rather than looking at sparser windows of the inputs (via strided convolutions) or averaging input patches, which could cause you to miss feature presence information or dilute it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Function: ReLU or Rectified Linear Unit**\n",
    "![](https://brohrer.github.io/images/cnn8.png)\n",
    "*Figure from [How CNNs work?](https://brohrer.github.io/how_convolutional_neural_networks_work.html)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
